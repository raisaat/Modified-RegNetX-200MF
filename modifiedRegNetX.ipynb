{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modifiedRegNetX.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP5GxBqpsOJh",
        "outputId": "5bab42f6-007b-484f-c1ec-1cadb92d705f"
      },
      "source": [
        "################################################################################\n",
        "#\n",
        "# LOGISTICS\n",
        "#\n",
        "#    Name: Raisaat Rashid\n",
        "#\n",
        "# DESCRIPTION\n",
        "#\n",
        "#    Image classification in PyTorch for ImageNet reduced to 100 classes and\n",
        "#    down sampled such that the short side is 64 pixels and the long side is\n",
        "#    >= 64 pixels\n",
        "#\n",
        "#    This script achieved a best accuracy of 67.32% on epoch 125 with a learning\n",
        "#    rate at that point of 0.001023 and time required for each epoch of ~ 117 s\n",
        "#\n",
        "# NOTES\n",
        "#\n",
        "#    0. For a mapping of category names to directory names see:\n",
        "#       https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57\n",
        "#\n",
        "#    1. The original 2012 ImageNet images are down sampled such that their short\n",
        "#       side is 64 pixels (the other side is >= 64 pixels) and only 100 of the\n",
        "#       original 1000 classes are kept.\n",
        "#\n",
        "#    2. Build and train a RegNetX image classifier modified as follows:\n",
        "#\n",
        "#       - Set stride = 1 (instead of stride = 2) in the stem\n",
        "#       - Replace the first stride = 2 down sampling building block in the\n",
        "#         original network by a stride = 1 normal building block\n",
        "#       - The fully connected layer in the decoder outputs 100 classes instead\n",
        "#         of 1000 classes\n",
        "#\n",
        "#       The original RegNetX takes in 3x224x224 input images and generates Nx7x7\n",
        "#       feature maps before the decoder, this modified RegNetX will take in\n",
        "#       3x56x56 input images and generate Nx7x7 feature maps before the decoder.\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# IMPORT\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn       as     nn\n",
        "import torch.optim    as     optim\n",
        "from   torch.autograd import Function\n",
        "\n",
        "# torch utils\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# additional libraries\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import time\n",
        "import math\n",
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# PARAMETERS\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# data\n",
        "DATA_DIR_1        = 'data'\n",
        "DATA_DIR_2        = 'data/imagenet64'\n",
        "DATA_DIR_TRAIN    = 'data/imagenet64/train'\n",
        "DATA_DIR_TEST     = 'data/imagenet64/val'\n",
        "DATA_FILE_TRAIN_1 = 'Train1.zip'\n",
        "DATA_FILE_TRAIN_2 = 'Train2.zip'\n",
        "DATA_FILE_TRAIN_3 = 'Train3.zip'\n",
        "DATA_FILE_TRAIN_4 = 'Train4.zip'\n",
        "DATA_FILE_TRAIN_5 = 'Train5.zip'\n",
        "DATA_FILE_TEST_1  = 'Val1.zip'\n",
        "DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'\n",
        "DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'\n",
        "DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'\n",
        "DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'\n",
        "DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'\n",
        "DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'\n",
        "DATA_BATCH_SIZE   = 512\n",
        "DATA_NUM_WORKERS  = 4\n",
        "DATA_NUM_CHANNELS = 3\n",
        "DATA_NUM_CLASSES  = 100\n",
        "DATA_RESIZE       = 64\n",
        "DATA_CROP         = 56\n",
        "DATA_MEAN         = (0.485, 0.456, 0.406)\n",
        "DATA_STD_DEV      = (0.229, 0.224, 0.225)\n",
        "\n",
        "# model parameters\n",
        "MODEL_LEVEL_WIDTHS = [24, 56, 152, 368] # Width/output channels of each of the 4 levels in the encoder body\n",
        "MODEL_LEVEL_DEPTHS = [1, 1, 4, 7] # Depth of/number of blocks in each of the 4 levels in the encoder body\n",
        "MODEL_BOTTLENECK_RATIO = 1\n",
        "MODEL_GROUP_SIZE = 8\n",
        "MODEL_STEM_WIDTH = 24\n",
        "MODEL_CONV3_FILTER_FR = 3\n",
        "MODEL_CONV3_FILTER_FC = 3\n",
        "\n",
        "# training (linear warm up with cosine decay learning rate)\n",
        "TRAINING_LR_MAX          = 0.001\n",
        "TRAINING_LR_INIT_SCALE   = 0.01\n",
        "TRAINING_LR_INIT_EPOCHS  = 5\n",
        "TRAINING_LR_FINAL_SCALE  = 0.01\n",
        "TRAINING_LR_FINAL_EPOCHS = 120\n",
        "TRAINING_NUM_EPOCHS      = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
        "TRAINING_LR_INIT         = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
        "TRAINING_LR_FINAL        = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
        "\n",
        "# file parameters\n",
        "FILE_NAME = 'Model.pt'\n",
        "FILE_SAVE = 1\n",
        "FILE_LOAD = 0\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# DATA\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# create a local directory structure for data storage\n",
        "if (os.path.exists(DATA_DIR_1) == False):\n",
        "    os.mkdir(DATA_DIR_1)\n",
        "if (os.path.exists(DATA_DIR_2) == False):\n",
        "    os.mkdir(DATA_DIR_2)\n",
        "if (os.path.exists(DATA_DIR_TRAIN) == False):\n",
        "    os.mkdir(DATA_DIR_TRAIN)\n",
        "if (os.path.exists(DATA_DIR_TEST) == False):\n",
        "    os.mkdir(DATA_DIR_TEST)\n",
        "\n",
        "# download data\n",
        "if (os.path.exists(DATA_FILE_TRAIN_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_2) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_3) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_4) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_5) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)\n",
        "if (os.path.exists(DATA_FILE_TEST_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)\n",
        "\n",
        "# extract data\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TEST)\n",
        "\n",
        "# transforms\n",
        "transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "\n",
        "# data sets\n",
        "dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)\n",
        "dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)\n",
        "\n",
        "# data loader\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True,  num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False, num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# NETWORK BUILDING BLOCK\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# X block\n",
        "class XBlock(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    def __init__(self, Ni, No, Fr, Fc, Sr, Sc, G):\n",
        "\n",
        "        # parent initialization\n",
        "        super(XBlock, self).__init__()\n",
        "        \n",
        "        self.downsample = False\n",
        "\n",
        "        # identity\n",
        "        if Sr != 1 or Ni != No:\n",
        "            self.downsample = True\n",
        "            self.conv0 = nn.Conv2d(Ni, No, kernel_size=1, stride=(Sr, Sc), bias=False)\n",
        "            self.bn0 = nn.BatchNorm2d(No)\n",
        "        \n",
        "        # layers\n",
        "        self.conv1 = nn.Conv2d(Ni, Ni, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(Ni)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(Ni, Ni, kernel_size=(Fr, Fc), stride=(Sr, Sc), padding=1, groups = Ni // G, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(Ni)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(Ni, No, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(No)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "        # residual\n",
        "        res = self.conv1(x)\n",
        "        res = self.bn1(res)\n",
        "        res = self.relu1(res)\n",
        "        \n",
        "        res = self.conv2(res)\n",
        "        res = self.bn2(res)\n",
        "        res = self.relu2(res)\n",
        "\n",
        "        res = self.conv3(res)\n",
        "        res = self.bn3(res)\n",
        "        \n",
        "        # identity\n",
        "        if self.downsample:\n",
        "            x = self.conv0(x)\n",
        "            x = self.bn0(x)\n",
        "\n",
        "        y = res + x\n",
        "        y = self.relu3(y)\n",
        "\n",
        "        # return\n",
        "        return y\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# NETWORK\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# define\n",
        "class Model(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    def __init__(self, data_num_channels, \n",
        "                 w, d, g, stem_width, filter_Fr, filter_Fc,\n",
        "                 data_num_classes):\n",
        "\n",
        "        # parent initialization\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        # encoder stem\n",
        "        self.stem = nn.ModuleList()\n",
        "        self.stem.append(nn.Conv2d(data_num_channels, stem_width, kernel_size=3, padding=1, bias=False))\n",
        "        self.stem.append(nn.BatchNorm2d(stem_width))\n",
        "        self.stem.append(nn.ReLU())\n",
        "      \n",
        "        # encoder body - level 1\n",
        "        self.enc_1 = nn.ModuleList()\n",
        "        self.enc_1.append(XBlock(stem_width, w[0], filter_Fr, filter_Fc, 1, 1, g))\n",
        "        for i in range(d[0] - 1):\n",
        "          self.enc_1.append(XBlock(w[0], w[0], filter_Fr, filter_Fc, 1, 1, g))\n",
        "\n",
        "        # encoder body - level 2\n",
        "        self.enc_2 = nn.ModuleList()\n",
        "        self.enc_2.append(XBlock(w[0], w[1], filter_Fr, filter_Fc, 2, 2, g))\n",
        "        for i in range(d[1] - 1):\n",
        "          self.enc_2.append(XBlock(w[1], w[1], filter_Fr, filter_Fc, 1, 1, g))\n",
        "\n",
        "        # encoder body - level 3\n",
        "        self.enc_3 = nn.ModuleList()\n",
        "        self.enc_3.append(XBlock(w[1], w[2], filter_Fr, filter_Fc, 2, 2, g))\n",
        "        for i in range(d[2] - 1):\n",
        "          self.enc_3.append(XBlock(w[2], w[2], filter_Fr, filter_Fc, 1, 1, g))\n",
        "\n",
        "        # encoder body - level 4\n",
        "        self.enc_4 = nn.ModuleList()\n",
        "        self.enc_4.append(XBlock(w[2], w[3], filter_Fr, filter_Fc, 2, 2, g))\n",
        "        for i in range(d[3] - 1):\n",
        "          self.enc_4.append(XBlock(w[3], w[3], filter_Fr, filter_Fc, 1, 1, g))\n",
        "        \n",
        "        # decoder\n",
        "        self.dec = nn.ModuleList()\n",
        "        self.dec.append(nn.AdaptiveAvgPool2d(output_size=1))\n",
        "        self.dec.append(nn.Flatten())\n",
        "        self.dec.append(nn.Linear(w[3], data_num_classes))\n",
        "        \n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # forward propagation through the encoder stem\n",
        "        for layer in self.stem:\n",
        "            x = layer(x)\n",
        "\n",
        "        # forward propagation through encoder body - level 1\n",
        "        for layer in self.enc_1:\n",
        "            x = layer(x)\n",
        "\n",
        "        # forward propagation through encoder body - level 2\n",
        "        for layer in self.enc_2:\n",
        "            x = layer(x)\n",
        "        \n",
        "        # forward propagation through encoder body - level 3\n",
        "        for layer in self.enc_3:\n",
        "            x = layer(x)\n",
        "        \n",
        "        # forward propagation through encoder body - level 4\n",
        "        for layer in self.enc_4:\n",
        "            x = layer(x)\n",
        "                \n",
        "        # forward propagation through the decoder\n",
        "        for layer in self.dec:\n",
        "            x = layer(x)\n",
        "\n",
        "        y = x\n",
        "\n",
        "        # return\n",
        "        return y\n",
        "\n",
        "# create model\n",
        "model = Model(DATA_NUM_CHANNELS, MODEL_LEVEL_WIDTHS, MODEL_LEVEL_DEPTHS, MODEL_GROUP_SIZE, MODEL_STEM_WIDTH, MODEL_CONV3_FILTER_FR, MODEL_CONV3_FILTER_FC, DATA_NUM_CLASSES)\n",
        "\n",
        "# specify the device as the GPU if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# enable data parallelization for multi GPU systems\n",
        "if (torch.cuda.device_count() > 1):\n",
        "   model = nn.DataParallel(model)\n",
        "print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)\n",
        "\n",
        "# transfer the network to the device\n",
        "model.to(device)\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# ERROR AND OPTIMIZER\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# error (softmax cross entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# TRAINING\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# start epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "\n",
        "    # linear warmup followed by cosine decay\n",
        "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
        "    else:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
        "\n",
        "    return lr\n",
        "\n",
        "# model loading\n",
        "if FILE_LOAD == 1:\n",
        "    checkpoint = torch.load(FILE_NAME)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "# cycle through the epochs\n",
        "epochs = []\n",
        "accuracies = []\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "for epoch in range(start_epoch, TRAINING_NUM_EPOCHS):\n",
        "    epochs.append(epoch)\n",
        "\n",
        "    # initialize train set statistics\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    num_batches   = 0\n",
        "\n",
        "    # set the learning rate for the epoch\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr_schedule(epoch)\n",
        "\n",
        "    # cycle through the train set\n",
        "    for data in dataloader_train:\n",
        "\n",
        "        # extract a batch of data and move it to the appropriate device\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass, loss, backward pass and weight update\n",
        "        outputs = model(inputs)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update statistics\n",
        "        training_loss = training_loss + loss.item()\n",
        "        num_batches   = num_batches + 1\n",
        "\n",
        "    # initialize test set statistics\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total   = 0\n",
        "\n",
        "    # no weight update / no gradient needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # cycle through the test set\n",
        "        for data in dataloader_test:\n",
        "\n",
        "            # extract a batch of data and move it to the appropriate device\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass and prediction\n",
        "            outputs      = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # update test set statistics\n",
        "            test_total   = test_total + labels.size(0)\n",
        "            test_correct = test_correct + (predicted == labels).sum().item()\n",
        "\n",
        "    # epoch statistics\n",
        "    accuracy = 100.0*test_correct/test_total\n",
        "    accuracies.append(accuracy)\n",
        "    loss = (training_loss/num_batches)/DATA_BATCH_SIZE\n",
        "    losses.append(loss)\n",
        "    print('Epoch {0:2d} lr = {1:8.6f} avg loss = {2:8.6f} accuracy = {3:5.2f}'.format(epoch, lr_schedule(epoch), loss, accuracy))\n",
        "    \n",
        "    # model saving\n",
        "    if FILE_SAVE == 1:\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict()\n",
        "      }, FILE_NAME)\n",
        "\n",
        "print(\"\\nTotal training time: %s minutes\" % ((time.time() - start_time)/60))\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# TEST\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# initialize test set statistics\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total   = 0\n",
        "\n",
        "# no weight update / no gradient needed\n",
        "with torch.no_grad():\n",
        "\n",
        "    # cycle through the test set\n",
        "    for data in dataloader_test:\n",
        "\n",
        "        # extract a batch of data and move it to the appropriate device\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # forward pass and prediction\n",
        "        outputs      = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # update test set statistics\n",
        "        test_total   = test_total + labels.size(0)\n",
        "        test_correct = test_correct + (predicted == labels).sum().item()\n",
        "\n",
        "# test set statistics\n",
        "print('Final accuracy of test set = {0:5.2f}'.format((100.0*test_correct/test_total)))\n",
        "print('')\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# DISPLAY\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# plot training data loss vs. epoch\n",
        "plot1 = plt.figure(1)\n",
        "plt.plot(epochs, losses)\n",
        "plt.title('Training data loss vs. epoch')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training data loss\")\n",
        "\n",
        "# plot training data accuracy vs. epoch\n",
        "plot1 = plt.figure(2)\n",
        "plt.plot(epochs, accuracies)\n",
        "plt.title('Training data accuracy vs. epoch')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training data accuracy (%)\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using 0 GPU(s)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}